{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fcafea",
      "metadata": {
        "id": "a0fcafea"
      },
      "outputs": [],
      "source": [
        "print(\"---starting with the hackathon---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "img_path = '/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/IMG_6107.PNG'\n",
        "img = Image.open(img_path)\n",
        "img_resized = img.resize((500, 500))\n",
        "\n",
        "display(img_resized)\n"
      ],
      "metadata": {
        "id": "l5rw3Ptau9vj"
      },
      "id": "l5rw3Ptau9vj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **File Cleaner v1.0**"
      ],
      "metadata": {
        "id": "7LnAQPTPV6a3"
      },
      "id": "7LnAQPTPV6a3"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Files to delete\n",
        "files_to_delete = [\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/Parsed_text/all_policy_chunks.jsonl\",\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/Faiss_Index/policy_chunks_faiss_index.bin\",\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/Faiss_Index/policy_chunks_metadata.json\",\n",
        "]\n",
        "\n",
        "# Folder to clear\n",
        "enpdf_folder = \"/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/EnPDF\"\n",
        "\n",
        "# Ask user for confirmation\n",
        "confirm = input(\"Type 'YES' to delete the files and clear EnPDF folder: \")\n",
        "\n",
        "if confirm.strip().upper() == 'YES':\n",
        "    # Delete individual files\n",
        "    for file_path in files_to_delete:\n",
        "        if os.path.isfile(file_path):\n",
        "            try:\n",
        "                os.remove(file_path)\n",
        "                print(f\"Deleted: {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error deleting {file_path}: {e}\")\n",
        "        else:\n",
        "            print(f\"Not found: {file_path}\")\n",
        "\n",
        "    # Delete files from EnPDF folder\n",
        "    if os.path.exists(enpdf_folder):\n",
        "        for fname in os.listdir(enpdf_folder):\n",
        "            fpath = os.path.join(enpdf_folder, fname)\n",
        "            if os.path.isfile(fpath):\n",
        "                try:\n",
        "                    os.remove(fpath)\n",
        "                    print(f\"Deleted from EnPDF: {fpath}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to delete {fpath}: {e}\")\n",
        "    else:\n",
        "        print(f\"EnPDF folder not found: {enpdf_folder}\")\n",
        "else:\n",
        "    print(\"Deletion cancelled.\")\n"
      ],
      "metadata": {
        "id": "vZoLzHVzT6qp"
      },
      "id": "vZoLzHVzT6qp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**api handling**"
      ],
      "metadata": {
        "id": "DUYDCCYBWFNi"
      },
      "id": "DUYDCCYBWFNi"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install python-dotenv\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "env_path = r\"/content/drive/MyDrive/Colab Notebooks/BajajFinserv/.env\"\n",
        "\n",
        "load_dotenv(dotenv_path=env_path)\n",
        "\n",
        "api_key = os.getenv(\"API_KEY\")\n",
        "print(\"API Key Loaded:\", api_key is not None)  # Avoid printing actual key!\n"
      ],
      "metadata": {
        "id": "Dy47C-p9Y68z"
      },
      "id": "Dy47C-p9Y68z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parser**"
      ],
      "metadata": {
        "id": "eK_Cxh2XqiKo"
      },
      "id": "eK_Cxh2XqiKo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parser v1.0**\n"
      ],
      "metadata": {
        "id": "GkY1zzjPRZFJ"
      },
      "id": "GkY1zzjPRZFJ"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pypdf\n",
        "import os\n",
        "import pypdf\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from typing import List, Dict, Any\n",
        "import json # For saving chunks as JSON Lines\n",
        "from google.colab import drive, files # Import files for user uploads\n",
        "import shutil # For moving files\n",
        "\n",
        "# --- Function to parse and chunk PDF (no change needed here) ---\n",
        "def parse_and_chunk_pdf(file_path: str) -> List[Dict[str, Any]]:\n",
        "    all_page_texts = []\n",
        "    try:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            reader = pypdf.PdfReader(f)\n",
        "            num_pages = len(reader.pages)\n",
        "\n",
        "            for i, page in enumerate(reader.pages):\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    all_page_texts.append({\"text\": page_text, \"page_number\": i + 1})\n",
        "\n",
        "        if not all_page_texts:\n",
        "            print(f\"Warning: No text extracted from {file_path}. It might be an image-only PDF or empty.\")\n",
        "            return []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF file {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "\n",
        "    processed_chunks = []\n",
        "    for page_info in all_page_texts:\n",
        "        page_content = page_info[\"text\"]\n",
        "        page_number = page_info[\"page_number\"]\n",
        "\n",
        "        chunks_from_page = text_splitter.split_text(page_content)\n",
        "\n",
        "        for i, chunk_content in enumerate(chunks_from_page):\n",
        "            clean_chunk = ' '.join(chunk_content.split()).strip()\n",
        "            if clean_chunk:\n",
        "                processed_chunks.append({\n",
        "                    \"content\": clean_chunk,\n",
        "                    \"metadata\": {\n",
        "                        \"page_number\": page_number,\n",
        "                        \"source_file\": os.path.basename(file_path),\n",
        "                    }\n",
        "                })\n",
        "\n",
        "    return processed_chunks\n",
        "\n",
        "# --- Main execution block for handling multiple PDFs and storing chunks ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted.\")\n",
        "\n",
        "    # Define the base directory for your PDF files on Google Drive\n",
        "    PDF_INPUT_FOLDER = '/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/EnPDF/'\n",
        "    # Define the output directory for storing the single aggregated JSONL file\n",
        "    PARSED_TEXT_OUTPUT_FOLDER = '/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/Parsed_text/'\n",
        "    # Define the filename for the single aggregated chunks file\n",
        "    AGGREGATED_CHUNKS_FILENAME = 'all_policy_chunks.jsonl'\n",
        "    full_output_jsonl_path = os.path.join(PARSED_TEXT_OUTPUT_FOLDER, AGGREGATED_CHUNKS_FILENAME)\n",
        "\n",
        "    # Create the input and output folders if they don't exist\n",
        "    os.makedirs(PDF_INPUT_FOLDER, exist_ok=True)\n",
        "    os.makedirs(PARSED_TEXT_OUTPUT_FOLDER, exist_ok=True)\n",
        "    print(f\"PDF Input Folder: {PDF_INPUT_FOLDER}\")\n",
        "    print(f\"Parsed Text Output Folder: {PARSED_TEXT_OUTPUT_FOLDER}\")\n",
        "\n",
        "    # --- User PDF Upload Section ---\n",
        "    print(\"\\n--- Upload your PDF files ---\")\n",
        "    print(\"You can upload multiple PDF files.\")\n",
        "    print(\"After selecting files, click 'Done' or simply close the upload dialog.\")\n",
        "\n",
        "    uploaded = files.upload() # This opens a file picker dialog\n",
        "\n",
        "    if uploaded:\n",
        "        print(\"\\nMoving uploaded files to Google Drive...\")\n",
        "        for filename in uploaded.keys():\n",
        "            destination_path = os.path.join(PDF_INPUT_FOLDER, filename)\n",
        "            with open(destination_path, 'wb') as f:\n",
        "                f.write(uploaded[filename])\n",
        "            print(f\"Moved '{filename}' to '{PDF_INPUT_FOLDER}'\")\n",
        "    else:\n",
        "        print(\"No files were uploaded. Proceeding with existing PDFs in the folder (if any).\")\n",
        "    # --- End of User PDF Upload Section ---\n",
        "\n",
        "\n",
        "    all_document_chunks_aggregated: List[Dict[str, Any]] = []\n",
        "\n",
        "    if os.path.isdir(PDF_INPUT_FOLDER):\n",
        "        print(f\"\\n--- Processing PDFs from: {PDF_INPUT_FOLDER} ---\")\n",
        "        pdf_files = [f for f in os.listdir(PDF_INPUT_FOLDER) if f.lower().endswith('.pdf')]\n",
        "\n",
        "        if not pdf_files:\n",
        "            print(f\"No PDF files found in {PDF_INPUT_FOLDER}. Please upload some or place them manually.\")\n",
        "        else:\n",
        "            for pdf_file_name in pdf_files:\n",
        "                full_pdf_path = os.path.join(PDF_INPUT_FOLDER, pdf_file_name)\n",
        "\n",
        "                print(f\"\\n--- Processing {pdf_file_name} ---\")\n",
        "                try:\n",
        "                    chunks_from_current_pdf = parse_and_chunk_pdf(full_pdf_path)\n",
        "\n",
        "                    if chunks_from_current_pdf:\n",
        "                        print(f\"  Extracted {len(chunks_from_current_pdf)} chunks from {pdf_file_name}.\")\n",
        "                        all_document_chunks_aggregated.extend(chunks_from_current_pdf)\n",
        "                    else:\n",
        "                        print(f\"  No chunks extracted from {pdf_file_name}.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Skipping {pdf_file_name} due to error: {e}\")\n",
        "\n",
        "            print(f\"\\n--- Finished processing all PDFs ---\")\n",
        "            print(f\"Total chunks aggregated from all documents: {len(all_document_chunks_aggregated)}\")\n",
        "\n",
        "            # Save all aggregated chunks to a single JSON Lines file\n",
        "            if all_document_chunks_aggregated:\n",
        "                with open(full_output_jsonl_path, 'w', encoding='utf-8') as outfile:\n",
        "                    for chunk in all_document_chunks_aggregated:\n",
        "                        outfile.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
        "                print(f\"All aggregated chunks saved to: {full_output_jsonl_path}\")\n",
        "            else:\n",
        "                print(\"No chunks were aggregated to save.\")\n",
        "\n",
        "\n",
        "            print(\"\\n--- Sample of aggregated chunks (first 5 across all docs) ---\")\n",
        "            for i, chunk in enumerate(all_document_chunks_aggregated):\n",
        "                if i < 5:\n",
        "                    print(f\"--- Chunk {i+1} (Source: {chunk['metadata']['source_file']}, Page {chunk['metadata']['page_number']}) ---\")\n",
        "                    print(chunk['content'])\n",
        "                    print(\"-\" * 20)\n",
        "                else:\n",
        "                    break\n",
        "            if len(all_document_chunks_aggregated) > 5:\n",
        "                print(\"...\")\n",
        "                print(\"Only first 5 combined chunks printed for brevity. The 'all_document_chunks_aggregated' list holds all of them.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: Directory not found at {PDF_INPUT_FOLDER}.\")\n",
        "        print(\"Please check the path and ensure your Google Drive is mounted correctly.\")"
      ],
      "metadata": {
        "id": "1m35IxFTsIeM"
      },
      "id": "1m35IxFTsIeM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Vectorizer v1.0**"
      ],
      "metadata": {
        "id": "1pTHTsmoGIxn"
      },
      "id": "1pTHTsmoGIxn"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers sentence-transformers faiss-cpu # Install necessary libraries\n",
        "\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to your aggregated chunks file\n",
        "PARSED_TEXT_OUTPUT_FOLDER = '/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/Parsed_text/'\n",
        "AGGREGATED_CHUNKS_FILENAME = 'all_policy_chunks.jsonl'\n",
        "full_chunks_jsonl_path = os.path.join(PARSED_TEXT_OUTPUT_FOLDER, AGGREGATED_CHUNKS_FILENAME)\n",
        "\n",
        "# Path to save the FAISS index\n",
        "FAISS_INDEX_OUTPUT_FOLDER = '/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/Faiss_Index/'\n",
        "FAISS_INDEX_FILENAME = 'policy_chunks_faiss_index.bin' # Binary file for FAISS index\n",
        "FAISS_METADATA_FILENAME = 'policy_chunks_metadata.json' # JSON for metadata (chunk content, page, source)\n",
        "\n",
        "full_faiss_index_path = os.path.join(FAISS_INDEX_OUTPUT_FOLDER, FAISS_INDEX_FILENAME)\n",
        "full_faiss_metadata_path = os.path.join(FAISS_INDEX_OUTPUT_FOLDER, FAISS_METADATA_FILENAME)\n",
        "\n",
        "# Create the FAISS output folder if it doesn't exist\n",
        "os.makedirs(FAISS_INDEX_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- Load Embedding Model ---\n",
        "print(\"Loading Sentence Transformer model...\")\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# --- Step 1: Load Chunks ---\n",
        "print(f\"Loading chunks from {full_chunks_jsonl_path}...\")\n",
        "all_chunks: List[Dict[str, Any]] = []\n",
        "if os.path.exists(full_chunks_jsonl_path):\n",
        "    with open(full_chunks_jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            all_chunks.append(json.loads(line))\n",
        "    print(f\"Loaded {len(all_chunks)} chunks.\")\n",
        "else:\n",
        "    print(f\"Error: Chunks file not found at {full_chunks_jsonl_path}. Please run the PDF parsing script first.\")\n",
        "    exit() # Exit if chunks are not found\n",
        "\n",
        "# Extract content for embedding\n",
        "chunk_contents: List[str] = [chunk['content'] for chunk in all_chunks]\n",
        "chunk_metadatas: List[Dict[str, Any]] = [\n",
        "    {\n",
        "        \"source_file\": chunk['metadata']['source_file'],\n",
        "        \"page_number\": chunk['metadata']['page_number'],\n",
        "        \"content\": chunk['content'] # Store content here for easy retrieval\n",
        "    }\n",
        "    for chunk in all_chunks\n",
        "]\n",
        "\n",
        "# --- Step 2 & 3: Generate Embeddings ---\n",
        "print(f\"Generating embeddings for {len(chunk_contents)} chunks...\")\n",
        "chunk_embeddings = embedding_model.encode(chunk_contents, show_progress_bar=True)\n",
        "print(\"Embeddings generated.\")\n",
        "\n",
        "# Ensure embeddings are in float32 for FAISS\n",
        "chunk_embeddings = np.array(chunk_embeddings).astype('float32')\n",
        "\n",
        "# --- Step 4 & 5: Initialize and Populate FAISS Index ---\n",
        "embedding_dimension = chunk_embeddings.shape[1]\n",
        "print(f\"Embedding dimension: {embedding_dimension}\")\n",
        "\n",
        "print(\"Initializing FAISS index...\")\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "print(\"Adding embeddings to FAISS index...\")\n",
        "index.add(chunk_embeddings)\n",
        "print(f\"FAISS index contains {index.ntotal} vectors.\")\n",
        "\n",
        "# --- Step 6: Save the FAISS Index and Metadata ---\n",
        "print(f\"Saving FAISS index to {full_faiss_index_path}...\")\n",
        "faiss.write_index(index, full_faiss_index_path)\n",
        "print(\"FAISS index saved.\")\n",
        "\n",
        "print(f\"Saving chunk metadata to {full_faiss_metadata_path}...\")\n",
        "with open(full_faiss_metadata_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(chunk_metadatas, f, ensure_ascii=False, indent=2)\n",
        "print(\"Metadata saved.\")\n",
        "\n",
        "print(\"\\n--- FAISS Indexing Complete ---\")\n",
        "print(f\"Total chunks indexed: {index.ntotal}\")\n",
        "print(\"The FAISS index and its corresponding metadata have been saved. They are now ready for retrieval when a user query is provided.\")"
      ],
      "metadata": {
        "id": "X4Wsu21lKE_C"
      },
      "id": "X4Wsu21lKE_C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Final query handler**\n",
        "# **The Query Block is divided in to two blocks**\n",
        "1.    **spell checker and grammatical mistake correction**\n",
        "2.    **the evaluator**\n"
      ],
      "metadata": {
        "id": "2gVI9vnllKQV"
      },
      "id": "2gVI9vnllKQV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install openai python-dotenv faiss-cpu sentence-transformers --upgrade -q\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Dict, Any\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio for async functions in Jupyter/Colab environments\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Google Drive Mount and .env loading ---\n",
        "from google.colab import drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "DOTENV_PATH = '/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/.env'\n",
        "if os.path.exists(DOTENV_PATH):\n",
        "    load_dotenv(dotenv_path=DOTENV_PATH)\n",
        "    print(f\"Loaded .env file from: {DOTENV_PATH}\")\n",
        "else:\n",
        "    print(f\"Error: .env file not found at {DOTENV_PATH}. Please check the path and ensure it's accessible.\")\n",
        "    exit(\"Exiting: .env file not found.\") # Critical dependency\n",
        "\n",
        "# --- Configure for Together.ai using OpenAI client ---\n",
        "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
        "\n",
        "if not TOGETHER_API_KEY:\n",
        "    print(\"WARNING: TOGETHER_API_KEY not found in environment variables. Please set it in your .env file.\")\n",
        "    exit(\"Exiting: TOGETHER_API_KEY is not set.\") # Critical dependency\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=TOGETHER_API_KEY,\n",
        "    base_url=\"https://api.together.xyz/v1/\"\n",
        ")\n",
        "\n",
        "LLM_MODEL_TOGETHER = \"mistralai/Mistral-7B-Instruct-v0.2\" # Using the model you specified\n",
        "\n",
        "# --- FAISS & Embedding Model Configuration ---\n",
        "FAISS_INDEX_PATH = '/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/Faiss_Index/policy_chunks_faiss_index.bin'\n",
        "METADATA_PATH = '/content/drive/MyDrive/Colab Notebooks/BajajFinserv_A/Faiss_Index/policy_chunks_metadata.json'\n",
        "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "TOP_K_RETRIEVAL = 5 # Number of chunks to retrieve for RAG\n",
        "\n",
        "# --- Load FAISS Index ---\n",
        "print(\"Loading FAISS index...\")\n",
        "try:\n",
        "    faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
        "    # Ensure the index is suitable for dot product similarity (IP) if your embeddings are normalized\n",
        "    if not isinstance(faiss_index, faiss.IndexFlatIP):\n",
        "        embedding_dim = faiss_index.d\n",
        "        ip_index = faiss.IndexFlatIP(embedding_dim)\n",
        "        ip_index.add(faiss_index.reconstruct_n(0, faiss_index.ntotal))\n",
        "        faiss_index = ip_index\n",
        "    print(f\"FAISS index loaded with {faiss_index.ntotal} vectors and converted to IndexFlatIP.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading FAISS index: {e}\")\n",
        "    exit(\"Exiting: FAISS index failed to load. Please check the path and file.\")\n",
        "\n",
        "# --- Load Metadata ---\n",
        "print(\"Loading metadata...\")\n",
        "try:\n",
        "    with open(METADATA_PATH, 'r', encoding='utf-8') as f:\n",
        "        metadata = json.load(f)\n",
        "    print(f\"Metadata loaded with {len(metadata)} entries.\")\n",
        "    # --- IMPORTANT: VERIFY YOUR METADATA KEYS HERE ---\n",
        "    if metadata:\n",
        "        sample_chunk = metadata[0]\n",
        "        print(\"\\n--- Sample Metadata Chunk Keys (VERIFY THESE!) ---\")\n",
        "        for key in sample_chunk.keys():\n",
        "            print(f\"- {key}: {sample_chunk[key]}\")\n",
        "        print(\"--------------------------------------------------\\n\")\n",
        "        print(\"Expected keys for display: 'source_document' (or similar), 'page_number' (or 'page'), 'id' (or 'chunk_id'), 'content' (or 'text')\")\n",
        "    else:\n",
        "        print(\"Metadata is empty. No chunks to retrieve.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading metadata: {e}\")\n",
        "    exit(\"Exiting: Metadata failed to load. Please check the path and file.\")\n",
        "\n",
        "# --- Load Embedding Model ---\n",
        "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "print(f\"Embedding model '{EMBEDDING_MODEL_NAME}' loaded.\")\n",
        "\n",
        "# --- Normalize function for embeddings (important for IP similarity) ---\n",
        "def normalize_vectors(vecs: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Normalizes a NumPy array of vectors for dot product similarity.\"\"\"\n",
        "    norms = np.linalg.norm(vecs, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1e-12 # Prevent division by zero\n",
        "    return vecs / norms\n",
        "\n",
        "# --- Semantic Cleaning ---\n",
        "def clean_query(query: str) -> str:\n",
        "    cleaned_text = ''.join(char for char in query if char.isalnum() or char.isspace()).strip()\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
        "    return cleaned_text\n",
        "\n",
        "# --- Top-k Retriever ---\n",
        "def retrieve_top_k(query_for_embedding: str, k: int = TOP_K_RETRIEVAL) -> List[Dict[str, Any]]:\n",
        "    if faiss_index.ntotal == 0:\n",
        "        print(\"Warning: FAISS index is empty. Cannot perform search.\")\n",
        "        return []\n",
        "\n",
        "    query_vector = embedder.encode([query_for_embedding]).astype(\"float32\")\n",
        "    query_vector = normalize_vectors(query_vector) # Normalize query vector\n",
        "\n",
        "    D, I = faiss_index.search(query_vector, k)\n",
        "\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        if 0 <= idx < len(metadata):\n",
        "            chunk = metadata[idx].copy() # Use .copy() to avoid modifying original metadata\n",
        "            chunk['retrieval_score'] = float(D[0, np.where(I[0] == idx)[0][0]])\n",
        "            results.append(chunk)\n",
        "        else:\n",
        "            print(f\"Warning: Retrieved index {idx} is out of bounds for metadata list (size {len(metadata)}).\")\n",
        "    return results\n",
        "\n",
        "# --- LLM API Call Function ---\n",
        "async def call_llm_api(prompt_messages: List[Dict[str, str]], json_output: bool = True) -> str:\n",
        "    if not TOGETHER_API_KEY:\n",
        "        raise ValueError(\"Together.ai API key is not set. Cannot call LLM API.\")\n",
        "\n",
        "    payload = {\n",
        "        \"model\": LLM_MODEL_TOGETHER,\n",
        "        \"messages\": prompt_messages,\n",
        "        \"temperature\": 0.2,\n",
        "        \"max_tokens\": 1024,\n",
        "    }\n",
        "    if json_output:\n",
        "        payload[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(**payload)\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM API call: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- Entity Extraction Function ---\n",
        "async def enhance_and_extract_query_with_llm(raw_query: str) -> Dict[str, Any]:\n",
        "    prompt = f\"\"\"\n",
        "You are an intelligent query processing agent for an information retrieval system, especially good at understanding details relevant to policies, contracts, or general inquiries. Your task is to refine a user's natural language query and extract key information.\n",
        "\n",
        "Perform the following steps:\n",
        "1.  **Correct Spelling and Grammar:** Fix any spelling mistakes or grammatical errors in the query.\n",
        "2.  **Rephrase for Semantic Search:** Rephrase the query to be concise, grammatically correct, and semantically rich. This rephrasing should inherently consider synonyms and different ways of expressing the same intent, making it suitable for a vector-based semantic search system. Remove any informal greetings or conversational filler words.\n",
        "3.  **Extract Key Entities:** Identify and extract specific factual entities from the query. If the query implies an insurance or policy context, prioritize extracting relevant details. If not, extract general entities. **Only include entities that are explicitly mentioned or clearly implied.** If an entity type is not mentioned or found, it should be `null` and omitted from the final JSON.\n",
        "\n",
        "Expected Entity Types (use these keys if applicable, otherwise generalize):\n",
        "-   `person_name`: Name of a person.\n",
        "-   `age`: Age (e.g., \"46\", \"30 years old\").\n",
        "-   `gender`: Gender (e.g., \"Male\", \"Female\", \"M\", \"F\").\n",
        "-   `problem_description` / `procedure`: Medical condition, surgery, or incident (e.g., \"knee surgery\", \"warehouse fire\").\n",
        "-   `location`: Geographic location (e.g., \"Pune\", \"Delhi\").\n",
        "-   `policy_duration`: Duration or age of a policy (e.g., \"3-month policy\", \"1 year\").\n",
        "-   `document_type`: Type of document (e.g., \"policy\", \"contract\", \"email\").\n",
        "-   `date` / `time`: Specific date or duration (e.g., \"last week\", \"July 15, 2024\").\n",
        "-   `organization`: Name of a company or institution.\n",
        "-   `numerical_value`: Any other number with its context/unit (e.g., \"100 dollars\", \"5000 units\").\n",
        "-   `specific_term`: Any other important noun phrase or concept not fitting above (e.g., \"grace period\", \"instalments\", \"premium\").\n",
        "\n",
        "Here is the user's raw query:\n",
        "\"{raw_query}\"\n",
        "\n",
        "Provide your output as a JSON object with the following structure. Only include the \"extracted_entities\" fields that have a non-null value. The keys for extracted entities should match the `Expected Entity Types` where applicable, or be a descriptive general term.\n",
        "\n",
        "Example Output Structure (adapt keys based on query):\n",
        "{{\n",
        "    \"corrected_and_rephrased_query\": \"The grammatically correct and semantically enhanced query.\",\n",
        "    \"extracted_entities\": {{\n",
        "        \"age\": \"46\",\n",
        "        \"gender\": \"Male\",\n",
        "        \"procedure\": \"knee surgery\",\n",
        "        \"location\": \"Pune\",\n",
        "        \"policy_duration\": \"3-month policy\"\n",
        "    }}\n",
        "}}\n",
        "\"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    try:\n",
        "        llm_output_str = await call_llm_api(messages)\n",
        "        parsed_output = json.loads(llm_output_str)\n",
        "\n",
        "        if \"extracted_entities\" not in parsed_output or not isinstance(parsed_output[\"extracted_entities\"], dict):\n",
        "            parsed_output[\"extracted_entities\"] = {}\n",
        "\n",
        "        parsed_output[\"extracted_entities\"] = {k: v for k, v in parsed_output[\"extracted_entities\"].items() if v is not None}\n",
        "\n",
        "        return parsed_output\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM query enhancement. Returning basic fallback: {e}\")\n",
        "        return {\n",
        "            \"corrected_and_rephrased_query\": clean_query(raw_query),\n",
        "            \"extracted_entities\": {}\n",
        "        }\n",
        "\n",
        "# --- Query Processing Pipeline (orchestrates entity extraction and rephrasing) ---\n",
        "async def process_user_query_pipeline(raw_query: str) -> Dict[str, Any]:\n",
        "    llm_enhanced_output = await enhance_and_extract_query_with_llm(raw_query)\n",
        "    final_query_for_embedding = clean_query(\n",
        "        llm_enhanced_output.get(\"corrected_and_rephrased_query\", raw_query)\n",
        "    )\n",
        "    return {\n",
        "        \"original_query\": raw_query,\n",
        "        \"corrected_and_rephrased_query\": llm_enhanced_output.get(\"corrected_and_rephrased_query\", raw_query),\n",
        "        \"query_for_embedding\": final_query_for_embedding,\n",
        "        \"extracted_entities\": llm_enhanced_output.get(\"extracted_entities\", {})\n",
        "    }\n",
        "\n",
        "# --- Construct Prompt for LLM (Full RAG Response) ---\n",
        "def construct_rag_prompt(user_query: str, context_chunks: List[Dict[str, Any]], extracted_entities: Dict[str, Any]) -> str:\n",
        "    if not context_chunks:\n",
        "        return f\"\"\"You are a helpful assistant. No specific document context was found for your query. Please answer based on your general knowledge or state that you cannot find relevant information.\n",
        "USER QUERY: {user_query}\n",
        "Provide a comprehensive answer in clear, human-understandable language. If you cannot answer, state why.\n",
        "\"\"\"\n",
        "\n",
        "    context_text = \"\\n\\n\".join([\n",
        "        # --- IMPORTANT: ADJUST THESE KEYS TO MATCH YOUR METADATA.JSON ---\n",
        "        # Example: if your metadata has 'filename', 'page', 'id_chunk', use those instead of 'source_document', 'page_number', 'id'\n",
        "        f\"--- Document: {chunk.get('source_document', 'Unknown')}, Page: {chunk.get('page_number', 'N/A')}, Chunk ID: {chunk.get('id', 'N/A')} ---\\n{chunk.get('content') or chunk.get('text', '')}\"\n",
        "        for chunk in context_chunks\n",
        "    ])\n",
        "\n",
        "    entities_str = \"\\n\".join([f\"- {k.replace('_', ' ').title()}: {v}\" for k, v in extracted_entities.items()]) if extracted_entities else \"No specific entities extracted.\"\n",
        "\n",
        "    prompt = f\"\"\"You are an expert information retrieval and analysis assistant.\n",
        "Your task is to answer user queries truthfully and comprehensively using ONLY the provided \"DOCUMENT CONTEXT\".\n",
        "If the answer cannot be found in the context, clearly state \"I cannot find a direct answer in the provided documents.\" and then, if appropriate, provide general information.\n",
        "\n",
        "Strictly follow these instructions:\n",
        "1.  **Analyze**: Carefully read the \"USER QUERY\", \"EXTRACTED ENTITIES\", and \"DOCUMENT CONTEXT\".\n",
        "2.  **Formulate Answer**: Construct a detailed, clear, and human-understandable answer.\n",
        "3.  **Directness**: Your answer MUST be derived *directly* from the provided DOCUMENT CONTEXT. Do not invent information.\n",
        "4.  **Completeness**: Address all aspects of the user's query.\n",
        "5.  **Referencing**: For *every* piece of information you provide that comes from the context, explicitly reference the source using: \"[See Document: [Name], Page: [Page Number], Section: [ID]]\".\n",
        "6.  **Conditions/Exceptions**: Ensure to cover any relevant conditions, exceptions, or specific terms in the context that apply.\n",
        "7.  **Output Format**: Provide the final answer as a JSON object with this structure:\n",
        "    ```json\n",
        "    {{\n",
        "        \"Decision\": \"Information Provided\" | \"Cannot Determine\" | \"Specific Decision (e.g., Approved, Rejected, Policy Covered, Not Covered)\",\n",
        "        \"Amount\": \"N/A\" | \"$[amount]\" | \"Full Coverage\" | \"See Justification\",\n",
        "        \"Justification\": \"Your comprehensive answer derived from context, with source references. State if information is not in documents.\"\n",
        "    }}\n",
        "    ```\n",
        "\n",
        "USER QUERY:\n",
        "{user_query}\n",
        "\n",
        "EXTRACTED ENTITIES:\n",
        "{entities_str}\n",
        "\n",
        "DOCUMENT CONTEXT:\n",
        "{context_text}\n",
        "\n",
        "Your JSON response:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# --- Main Orchestration Function ---\n",
        "async def run_full_rag_pipeline() -> Dict[str, Any]:\n",
        "    user_query = input(\"Enter your query (e.g., '46M, knee surgery, Pune, 3-month policy' or 'what happens if premium is paid in instalments and not received within the grace period?'): \").strip()\n",
        "    if not user_query:\n",
        "        return {\"error\": \"Query cannot be empty.\"}\n",
        "\n",
        "    # Step 1: Query Processing and Entity Extraction\n",
        "    print(f\"\\n--- Step 1: Processing Query and Extracting Entities ---\")\n",
        "    processed_query_info = await process_user_query_pipeline(user_query)\n",
        "\n",
        "    original_query = processed_query_info[\"original_query\"]\n",
        "    rephrased_query = processed_query_info[\"corrected_and_rephrased_query\"]\n",
        "    query_for_embedding = processed_query_info[\"query_for_embedding\"]\n",
        "    extracted_entities = processed_query_info[\"extracted_entities\"]\n",
        "\n",
        "    print(f\"Original Query: '{original_query}'\")\n",
        "    print(f\"Rephrased for Semantic Search: '{rephrased_query}'\")\n",
        "    print(\"Processed Query (Extracted Entities):\")\n",
        "    if extracted_entities:\n",
        "        for key, value in extracted_entities.items():\n",
        "            print(f\"- {key.replace('_', ' ').title()}: {value}\")\n",
        "    else:\n",
        "        print(\"No specific entities extracted.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Step 2: Retrieve Relevant Documents\n",
        "    print(f\"\\n--- Step 2: Retrieving Top {TOP_K_RETRIEVAL} Relevant Documents ---\")\n",
        "    # First, retrieve based on the rephrased query for overall context\n",
        "    top_chunks = retrieve_top_k(query_for_embedding)\n",
        "\n",
        "    # Optional: Also retrieve based on individual entities if present, then combine/re-rank\n",
        "    if extracted_entities:\n",
        "        print(\"\\n🔎 Searching based on individual extracted entities (for detailed view):\")\n",
        "        for i, (entity_type, entity_value) in enumerate(extracted_entities.items()):\n",
        "            search_term = f\"{entity_value}\" # Use the entity value as the search term\n",
        "            entity_chunks = retrieve_top_k(search_term, k=TOP_K_RETRIEVAL)\n",
        "            print(f\"\\n🔎 Entity {i+1}: '{entity_type.replace('_', ' ').title()}: {entity_value}'\")\n",
        "            if entity_chunks:\n",
        "                for rank, chunk in enumerate(entity_chunks, 1):\n",
        "                    # --- IMPORTANT: ADJUST THESE KEYS TO MATCH YOUR METADATA.JSON ---\n",
        "                    # Example: if your metadata has 'filename', 'page', 'id_chunk', use those\n",
        "                    content_preview = chunk.get('content', chunk.get('text', ''))\n",
        "                    print(f\"  {rank}. [Score: {chunk.get('retrieval_score', 'N/A'):.4f}] Page {chunk.get('page_number', 'N/A')} - {chunk.get('source_document', 'Unknown')}\")\n",
        "                    print(f\"     Content: {content_preview[:120]}...\\n\") # Truncate for display\n",
        "                # top_chunks.extend(entity_chunks) # Uncomment this if you want to merge these for LLM context\n",
        "            else:\n",
        "                print(f\"  No specific matches found for entity '{entity_value}'.\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    if not top_chunks:\n",
        "        print(\"No relevant chunks found in the FAISS index for overall query.\")\n",
        "        final_response = {\n",
        "            \"Decision\": \"Cannot Determine\",\n",
        "            \"Amount\": \"N/A\",\n",
        "            \"Justification\": \"No relevant documents found for your query in the knowledge base.\"\n",
        "        }\n",
        "    else:\n",
        "        print(\"\\n--- Context for LLM Reasoning (from overall query retrieval) ---\")\n",
        "        for i, chunk in enumerate(top_chunks):\n",
        "            # --- IMPORTANT: ADJUST THESE KEYS TO MATCH YOUR METADATA.JSON ---\n",
        "            content_preview = chunk.get('content', chunk.get('text', ''))\n",
        "            print(f\"  {i+1}. Doc: {chunk.get('source_document', 'Unknown')}, Page: {chunk.get('page_number', 'N/A')}, ID: {chunk.get('id', 'N/A')}, Score: {chunk.get('retrieval_score', 'N/A'):.4f}, Content: {content_preview[:150]}...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Step 3: LLM Reasoning and Decision Making (RAG)\n",
        "        print(f\"\\n--- Step 3: Generating Final Answer with LLM (RAG) ---\")\n",
        "        rag_prompt = construct_rag_prompt(original_query, top_chunks, extracted_entities)\n",
        "        try:\n",
        "            response_content = await call_llm_api([\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert information retrieval and analysis assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": rag_prompt}\n",
        "            ], json_output=True)\n",
        "\n",
        "            final_response = json.loads(response_content)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON from final LLM response: {e}\")\n",
        "            print(f\"Raw LLM response: {response_content}\")\n",
        "            final_response = {\n",
        "                \"Decision\": \"Error\",\n",
        "                \"Amount\": \"N/A\",\n",
        "                \"Justification\": f\"LLM returned malformed JSON: {response_content}\"\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during final RAG response generation: {e}\")\n",
        "            final_response = {\n",
        "                \"Decision\": \"Error\",\n",
        "                \"Amount\": \"N/A\",\n",
        "                \"Justification\": f\"An unexpected error occurred during final answer generation: {e}\"\n",
        "            }\n",
        "\n",
        "    print(\"\\n--- Final RAG Decision ---\")\n",
        "    print(json.dumps(final_response, indent=2))\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "    return final_response # Return the final RAG response, not just entities\n",
        "\n",
        "# --- Run the Combined Pipeline ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting the full RAG system pipeline.\")\n",
        "    print(f\"Using Together.ai LLM: {LLM_MODEL_TOGETHER}\")\n",
        "    print(f\"Using Embedding Model: {EMBEDDING_MODEL_NAME}\")\n",
        "    print(f\"API Key path: {DOTENV_PATH}\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    async def main_interactive_loop():\n",
        "        while True:\n",
        "            try:\n",
        "                await run_full_rag_pipeline()\n",
        "\n",
        "                continue_prompt = input(\"Run another query? (yes/no): \").strip().lower()\n",
        "                if continue_prompt != 'yes':\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\"An unhandled error occurred in the main execution loop: {e}\")\n",
        "                break\n",
        "\n",
        "    asyncio.run(main_interactive_loop())"
      ],
      "metadata": {
        "id": "Gi_I-CkglHno"
      },
      "id": "Gi_I-CkglHno",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G4jMEZsFluJ7"
      },
      "id": "G4jMEZsFluJ7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}